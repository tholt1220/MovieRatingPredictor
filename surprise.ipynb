{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "import pandas as pd\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_ratings = pd.read_csv(\"train_ratings.csv\")\n",
    "val_ratings = pd.read_csv(\"val_ratings.csv\")\n",
    "test_ratings = pd.read_csv(\"test_ratings.csv\")\n",
    "\n",
    "shuffled_ratings = shuffle(train_ratings)\n",
    "\n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "\n",
    "train_data = Dataset.load_from_df(shuffled_ratings[['userId', 'movieId', 'rating']], reader=reader)\n",
    "trainset = train_data.build_full_trainset()\n",
    "\n",
    "val_data = Dataset.load_from_df(val_ratings[['userId', 'movieId', 'rating']], reader=reader)\n",
    "#valset = val_data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hyperparameter tuning\n",
    "# from surprise.model_selection import GridSearchCV\n",
    "# import time\n",
    "# start = time.time()\n",
    "\n",
    "# param_grid = {'n_epochs': [20, 30], 'lr_all': [0.002, 0.005, 0.01], 'reg_all': [0.0, 0.2, 0.4]}\n",
    "# gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5)\n",
    "\n",
    "# gs.fit(train_data)\n",
    "\n",
    "# # best RMSE score\n",
    "# print(gs.best_score['rmse'])\n",
    "\n",
    "# # combination of parameters that gave the best RMSE score\n",
    "# print(gs.best_params['rmse'])\n",
    "\n",
    "# print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "\n",
    "#fitting the training set\n",
    "algo = SVD()\n",
    "#algo = SVDpp() #uncomment and comment above line for SVDpp\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validating\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "#uses the validation set for validation\n",
    "junk, valset = train_test_split(val_data, train_size=1)\n",
    "evaluations = algo.test(valset)\n",
    "accuracy.rmse(evaluations, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "import csv\n",
    "csvfile = open('submission.csv', 'wb')\n",
    "csvwriter = csv.writer(csvfile)\n",
    "csvwriter.writerow([\"Id\", \"rating\"])\n",
    "\n",
    "for row in test_ratings.itertuples():\n",
    "    predictions = algo.predict(row.userId, row.movieId)\n",
    "    #round to nearest 0.5 (performed worse)\n",
    "    #rating = round(predictions[3] * 2) / 2\n",
    "    #csvwriter.writerow([row.Id, rating])\n",
    "    csvwriter.writerow([row.Id, predictions[3]])\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
